# Real-Time Person Removal & Inpainting System

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/OpenCV-4.x-green.svg" alt="OpenCV">
  <img src="https://img.shields.io/badge/YOLO-v11-orange.svg" alt="YOLO">
  <img src="https://img.shields.io/badge/LaMa-Inpainting-purple.svg" alt="LaMa">
</p>

A real-time video processing system that combines **person segmentation**, **facial recognition**, and **AI-powered inpainting** to seamlessly remove or replace people from live video feeds.

---

## ğŸ¬ RÃ©sultats

Voici des dÃ©monstrations du systÃ¨me en action :

### VidÃ©o "Dedans"

![Dedans](resultats/dedans.gif)

### VidÃ©o "Neige"

![Neige](resultats/neige.gif)

### Output Demo

![Output Demo](resultats/output_1765842783.gif)

---

## Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Keyboard Controls](#-keyboard-controls)
- [Configuration](#-configuration)
- [Technical Details](#-technical-details)
- [Project Structure](#-project-structure)
- [License](#-license)

---

##  Features

| Feature | Description |
|---------|-------------|
| **Person Segmentation** | YOLOv11-seg for accurate real-time human detection and segmentation |
|  **Facial Recognition** | Target-specific person removal using face_recognition library |
| **Dual Inpainting Modes** | Choose between fast OpenCV-based or AI-powered LaMa inpainting |
| **Real-Time Processing** | Optimized pipeline for live webcam feeds |
|  **Interactive Controls** | Toggle features on-the-fly with keyboard shortcuts |
| **Video Recording** | Automatic output recording in MP4 format |
|  **Multi-GPU Support** | CUDA, Intel XPU (OpenVINO), Apple MPS, and CPU fallback |

---

##  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Video Input   â”‚â”€â”€â”€â–¶â”‚  YOLOv11-seg     â”‚â”€â”€â”€â–¶â”‚  Human Masks    â”‚
â”‚   (Webcam)      â”‚    â”‚  Segmentation    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  Reference      â”‚â”€â”€â”€â–¶â”‚  Face           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  Image          â”‚    â”‚  Recognition     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Associate Face  â”‚
                       â”‚  with Body Mask  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                 â–¼                 â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Basic Mode    â”‚ â”‚   LaMa Mode    â”‚ â”‚   No Match   â”‚
     â”‚  (Background)  â”‚ â”‚  (AI Inpaint)  â”‚ â”‚  (Pass-thru) â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                  â”‚                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Video Output   â”‚
                       â”‚  (Display/Save)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended) or Intel GPU with OpenVINO
- Webcam

### Step 1: Clone the Repository

```bash
git clone https://github.com/Glenrunc/MI7_seg_impaint_deepfake.git
cd MI7_seg_impaint_deepfake
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
.\venv\Scripts\activate   # Windows
```

### Step 3: Install Dependencies

```bash
# Core dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install ultralytics opencv-python face_recognition numpy omegaconf pyyaml

# LaMa dependencies (from lama folder)
pip install -r lama/requirements.txt
```

### Step 4: Download Models

1. **YOLOv11-seg**: Automatically downloaded on first run, or place in `models/`
2. **LaMa Model**: Download the pre-trained model and place in `models/lama-places/big-lama-regular/`

```bash
# Create model directories
mkdir -p models/lama-places/big-lama-regular/models

# Download LaMa checkpoint (example)
# Place config.yaml and best.ckpt in the appropriate folders
```

---

##  Usage

### Basic Usage

```bash
python main.py
```

### With Custom Reference Image

Edit `main.py` to set your reference image:

```python
image_de_qqn = "data/your_reference_photo.jpg"
```

### With Custom Background

```python
background_image_path = "path/to/background.jpg"
```

---

## âŒ¨ï¸ Keyboard Controls

| Key | Action |
|-----|--------|
| `I` | Toggle inpainting ON/OFF |
| `L` | Switch to **LaMa** mode (AI-powered, higher quality) |
| `B` | Switch to **Basic** mode (faster, uses background image) |
| `R` | Refresh reference background image |
| `F` | Toggle facial recognition (target-specific vs all persons) |
| `Q` / `ESC` | Quit application |

---

## âš™ï¸ Configuration

### Inpainting Modes

| Mode | Speed | Quality | Requirements |
|------|-------|---------|--------------|
| **Basic** | âš¡âš¡âš¡ Fast | Medium | Background reference image |
| **LaMa** | âš¡ Slower | High | GPU recommended, LaMa model |

### FastLaMaInpainter Parameters

```python
lama_model = FastLaMaInpainter(
    model_path="models/lama-places/big-lama-regular",
    checkpoint="best.ckpt",
    max_resolution=512,  # Lower = faster, higher = better quality
    device="cuda"        # "cuda", "xpu", "mps", or "cpu"
)
```

### Face Recognition Tolerance

```python
# In extract_information()
tolerance=0.5  # Lower = stricter matching, Higher = more permissive
```

---

## Technical Details

### Pipeline Components

1. **YOLOv11-seg Segmentation**
   - Detects humans in frame (class 0)
   - Returns pixel-accurate segmentation masks
   - Supports OpenVINO INT8 quantization for Intel GPUs

2. **Face Recognition**
   - Uses `face_recognition` library (based on dlib)
   - Extracts 128-dimensional face encodings
   - Compares against reference person for selective removal

3. **Mask Association**
   - Links detected faces to their corresponding body masks
   - Calculates face centroid from facial landmarks
   - Creates separate face masks (above chin line)

4. **Inpainting Methods**
   - **Basic**: Direct background replacement using captured reference frame
   - **LaMa**: Large Mask Inpainting neural network for context-aware filling

### Performance Optimizations

- cuDNN benchmark mode enabled for CUDA
- TF32 acceleration on supported GPUs
- Resolution limiting for LaMa inference
- Automatic padding to multiples of 8 for FFT operations
- Half-precision (FP16) for YOLO inference

---

## Project Structure

```
MI7_seg_impaint_deepfake/
â”œâ”€â”€ main.py                 # Main application entry point
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolo11n-seg.pt      # YOLO segmentation model
â”‚   â””â”€â”€ lama-places/        # LaMa inpainting model
â”‚       â””â”€â”€ big-lama-regular/
â”‚           â”œâ”€â”€ config.yaml
â”‚           â””â”€â”€ models/
â”‚               â””â”€â”€ best.ckpt
â”œâ”€â”€ data/                   # Reference images
â”œâ”€â”€ output/                 # Recorded videos
â””â”€â”€ lama/                   # LaMa repository
    â”œâ”€â”€ saicinpainting/     # LaMa source code
    â”œâ”€â”€ configs/            # Training configs
    â””â”€â”€ requirements.txt
```

---

## Supported Hardware

| Platform | Segmentation | Inpainting | Notes |
|----------|--------------|------------|-------|
| NVIDIA CUDA | âœ… Full | âœ… Full | Recommended |
| Intel XPU (OpenVINO) | âœ… INT8 | âœ… Full | Good performance |
| Apple Silicon (MPS) | âœ… Full | âœ… Full | M1/M2 supported |
| CPU | âœ… Full | âœ… Full | Slower performance |

---

## Future Improvements

- [ ] Multi-person selective removal
- [ ] Temporal consistency for video inpainting
- [ ] GUI interface with parameter tuning
- [ ] Real-time background replacement with depth estimation
- [ ] Export to ONNX/TensorRT for deployment

---

## License

The LaMa inpainting model is licensed under Apache 2.0. See [lama/LICENSE](lama/LICENSE).

---
